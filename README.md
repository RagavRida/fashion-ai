# Fashion Reuse Studio ğŸŒ¿â™»ï¸

> **Production-grade AI fashion upcycling system** â€” Generate, redesign, and refine garments from text prompts or uploaded images, then get household DIY instructions to recreate designs in real life.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-orange.svg)](https://pytorch.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)

---

## âœ¨ System Capabilities

| Feature | Description |
|---------|-------------|
| **Prompt â†’ Fashion** | Generate 8 garment designs from a text description |
| **Image â†’ Redesign** | Upload any garment and get 8 AI upcycled variations |
| **Image + Prompt** | Combine reference image with specific redesign instructions |
| **Refinement Loop** | Chat-style iterative refinement: "make sleeves shorter", "add embroidery" |
| **DIY Guide** | LLM-generated household upcycling instructions for any AI design |

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Fashion Reuse Studio                         â”‚
â”‚                                                                     â”‚
â”‚  Frontend (React)           API (FastAPI)        Models              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Mode Tabs      â”‚â”€â”€â”€â”€â”€â”€â–¶ â”‚ /generate    â”‚â”€â”€â”€â–¶â”‚ SDXL + LoRA     â”‚ â”‚
â”‚  â”‚  Upload Zone    â”‚        â”‚ /redesign    â”‚â”€â”€â”€â–¶â”‚ ControlNet      â”‚ â”‚
â”‚  â”‚  Gallery /4up   â”‚â—€â”€â”€â”€â”€â”€â”€ â”‚ /redesign_p  â”‚â”€â”€â”€â–¶â”‚ IP-Adapter      â”‚ â”‚
â”‚  â”‚  Refine Chat    â”‚â”€â”€â”€â”€â”€â”€â–¶ â”‚ /refine      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚  DIY Panel      â”‚â—€â”€â”€â”€â”€â”€â”€ â”‚ /diy_guide   â”‚            â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                                  â”‚ Ranker (CLIP,   â”‚ â”‚
â”‚  Dataset Pipeline                                â”‚ Mask, Edge,     â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ LPIPS)          â”‚ â”‚
â”‚  â”‚ DeepFashion â†’ preprocess â†’ edges â†’      â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ masks â†’ prompts â†’ metadata.jsonl        â”‚              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                                  â”‚ DIY Guide LLM   â”‚ â”‚
â”‚  Training (4-Stage)                              â”‚ (GPT-4o/Claude  â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  /Ollama)        â”‚ â”‚
â”‚  â”‚ 1. GarmentUNet (segmentation)           â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ 2. SDXL LoRA (DeepFashion fine-tune)    â”‚                         â”‚
â”‚  â”‚ 3. ControlNet (Canny edge conditioning) â”‚                         â”‚
â”‚  â”‚ 4. IP-Adapter (reference conditioning)  â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions
- **LoRA first** (Phase 2) to learn fashion-domain priors, then ControlNet (Phase 3) loads LoRA weights
- **CFG-scale conditioning dropout** during ControlNet training for flexible inference
- **Multi-metric ranker**: CLIP relevance + mask IoU + edge correlation + aesthetic score âˆ’ LPIPS penalty
- **Fallback DIY guide** works without any LLM API key (pre-baked instructions)

---

## ğŸ“ Project Structure

```
fashion-ai/
â”œâ”€â”€ configs/              # All YAML configuration files
â”‚   â”œâ”€â”€ dataset.yaml
â”‚   â”œâ”€â”€ train_lora.yaml
â”‚   â”œâ”€â”€ train_controlnet.yaml
â”‚   â”œâ”€â”€ train_ip_adapter.yaml
â”‚   â”œâ”€â”€ train_segmentation.yaml
â”‚   â””â”€â”€ inference.yaml
â”œâ”€â”€ dataset_builder/      # Dataset processing pipeline
â”‚   â”œâ”€â”€ download_deepfashion.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ build_prompts.py
â”‚   â”œâ”€â”€ build_edges.py
â”‚   â”œâ”€â”€ build_masks.py
â”‚   â””â”€â”€ export_jsonl.py
â”œâ”€â”€ models/               # Model architecture code
â”‚   â”œâ”€â”€ segmentation/unet.py      # GarmentUNet
â”‚   â””â”€â”€ ranking/ranker.py         # CandidateRanker
â”œâ”€â”€ training/             # Training scripts + pipeline
â”‚   â”œâ”€â”€ train_segmentation.py
â”‚   â”œâ”€â”€ train_lora.py
â”‚   â”œâ”€â”€ train_controlnet.py
â”‚   â”œâ”€â”€ train_ip_adapter.py
â”‚   â””â”€â”€ automated_train_pipeline.sh
â”œâ”€â”€ inference/            # Inference engine
â”‚   â”œâ”€â”€ pipeline.py       # FashionPipeline (all modes)
â”‚   â””â”€â”€ diy_guide.py      # DIYGuideGenerator
â”œâ”€â”€ api/                  # FastAPI backend
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ schemas.py
â”œâ”€â”€ frontend/             # React UI
â”‚   â”œâ”€â”€ public/index.html
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx
â”‚       â”œâ”€â”€ index.js
â”‚       â””â”€â”€ index.css
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate.py       # FID, CLIP, LPIPS, IoU, DIY eval
â”œâ”€â”€ data/                 # (gitignored) data directory
â”œâ”€â”€ checkpoints/          # (gitignored) trained model weights
â”œâ”€â”€ outputs/              # (gitignored) generated images
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone and setup
git clone <your-repo>
cd fashion-ai

# Create conda env (Python 3.10+, CUDA 12.1)
conda create -n fashion-ai python=3.10 -y
conda activate fashion-ai
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

# Install Python dependencies
pip install -r requirements.txt

# Install accelerate + configure for your GPU
pip install accelerate
accelerate config    # Follow prompts for your GPU setup

# (Optional) Login to W&B for training logs
wandb login

# (Optional) Set up LLM API key for DIY guides
export OPENAI_API_KEY=sk-...
# or use local Ollama: set diy_guide.llm_provider: local in inference.yaml
```

### 2. Dataset Preparation

```bash
# Option A: Download DeepFashion from HuggingFace
python dataset_builder/download_deepfashion.py --source huggingface --output_dir data/raw

# One-shot preprocessing (all stages)
bash training/automated_train_pipeline.sh --prep-dataset --end-stage 0
```

### 3. Training (4-Stage Pipeline)

```bash
# Run all 4 training stages sequentially (GPU required)
bash training/automated_train_pipeline.sh --gpu-ids 0,1,2,3

# Or run individual stages:
# Stage 1: Segmentation CNN (~2h on A100)
accelerate launch training/train_segmentation.py --config configs/train_segmentation.yaml

# Stage 2: LoRA fine-tuning (~6h on A100)
accelerate launch training/train_lora.py --config configs/train_lora.yaml

# Stage 3: ControlNet (~8h on A100)
accelerate launch training/train_controlnet.py --config configs/train_controlnet.yaml

# Stage 4: IP-Adapter (~4h on A100)
accelerate launch training/train_ip_adapter.py --config configs/train_ip_adapter.yaml

# Resume from checkpoint (skips completed stages automatically)
bash training/automated_train_pipeline.sh --start-stage 3
```

### 4. Start the API Server

```bash
# Using pre-trained/public models (without fine-tuning)
python api/app.py
# or: uvicorn api.app:app --host 0.0.0.0 --port 8000 --reload

# API docs available at: http://localhost:8000/docs
```

### 5. Start the Frontend

```bash
cd frontend
npm install
npm start
# Open: http://localhost:3000
```

---

## ğŸ”Œ API Reference

### `GET /health`
System health check and GPU info.

### `POST /generate`
```json
{
  "prompt": "Upcycle a denim jacket into a cropped streetwear jacket with patches",
  "n_images": 4
}
```
Returns top 4 ranked images (base64 JPEG).

### `POST /redesign`
```json
{
  "image_b64": "<base64 garment image>",
  "n_images": 4
}
```
Returns 4 AI-generated redesign variations.

### `POST /redesign_prompt`
```json
{
  "image_b64": "<base64 garment image>",
  "prompt": "Convert into a formal blazer with gold buttons",
  "n_images": 4
}
```

### `POST /refine`
```json
{
  "previous_image_b64": "<base64 image>",
  "refinement_prompt": "Make sleeves shorter, add embroidery",
  "original_prompt": "denim jacket",
  "n_images": 4
}
```

### `POST /diy_guide`
```json
{
  "garment_category": "denim jacket",
  "edits_applied": ["cropped to waist", "added patches"],
  "style_description": "streetwear, urban",
  "difficulty_target": "Medium"
}
```
Returns step-by-step DIY instructions with materials, tools, steps, safety and budget tips.

---

## ğŸ“Š Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| FID | < 30 | vs DeepFashion2 val set |
| CLIP Score | > 0.28 | text-image alignment |
| LPIPS Diversity | > 0.3 | within each generation batch |
| Segmentation IoU | > 0.82 | on fashion dataset |
| DIY Guide Steps | â‰¥ 6 | all required fields present |
| Inference Speed | < 30s / 4 images | A100 GPU |

### Run Evaluation
```bash
python evaluation/evaluate.py \
  --config configs/inference.yaml \
  --eval_dir outputs/generated_samples \
  --real_dir data/processed/images_512 \
  --n_samples 200
```

---

## ğŸ”§ Configuration

Key configs in `configs/inference.yaml`:

```yaml
models:
  base_model: stabilityai/stable-diffusion-xl-base-1.0
  lora_weights: checkpoints/fashion_lora/fashion_lora.safetensors
  controlnet_weights: checkpoints/fashion_controlnet/
  
diy_guide:
  llm_provider: openai   # openai | anthropic | local (Ollama)
  openai_model: gpt-4o

generation:
  num_inference_steps: 50
  guidance_scale: 7.5
  num_images_per_prompt: 8
  top_k_return: 4
```

---

## ğŸ“š Documentation

- `TRAINING.md` â€” Detailed training guide for each stage
- `GPU_SETUP.md` â€” Multi-GPU setup, memory optimization
- `DEMO.md` â€” Interactive demo and API examples

---

## ğŸŒ± Sustainability

Fashion Reuse Studio is designed to **reduce textile waste** by:
- Making garment upcycling accessible to everyone
- Generating household-friendly DIY instructions (no industrial equipment)
- Providing budget-conscious material alternatives
- Demonstrating sustainability benefits for each transformation

---

## License
MIT License â€” see [LICENSE](LICENSE)
